{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7389ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bab61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import re\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def extract_from_page(page_number, page):\n",
    "    texte = page.extract_text()\n",
    "    if texte == \"\":\n",
    "        # Convertir l'image en niveaux de gris\n",
    "        gray = cv2.cvtColor(np.array(images[page_number]), cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Apply OCR to get the OSD (Orientation and Script Detection) information\n",
    "        data = pytesseract.image_to_osd(gray)\n",
    "        # Extract the rotation angle from the OSD data\n",
    "        lines = data.splitlines()\n",
    "        rotation_line = next((line for line in lines if line.startswith('Rotate:')), None)\n",
    "        rotation_angle = int(rotation_line.split(':')[1])\n",
    "        # Rotate the image to correct the text rotation\n",
    "        if rotation_angle != 0:\n",
    "            if rotation_angle == 90:\n",
    "                gray = cv2.rotate(gray, cv2.ROTATE_90_CLOCKWISE)\n",
    "            elif rotation_angle == 180:\n",
    "                gray = cv2.rotate(gray, cv2.ROTATE_180)\n",
    "            elif rotation_angle == 270:\n",
    "                gray = cv2.rotate(gray, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "                \n",
    "        # Appliquer des techniques de prétraitement d'image pour nettoyer les traces de stylos\n",
    "        # Par exemple, appliquer un filtre de flou pour réduire le bruit\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        # Appliquer une segmentation en seuillage adaptatif pour isoler le texte\n",
    "        _, threshold = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Effectuer OCR sur l'image traitée\n",
    "        texte = pytesseract.image_to_string(threshold, lang='eng', config='--psm 4')\n",
    "        \n",
    "    lines = texte.split(\"\\n\")\n",
    "    aev_words = []\n",
    "    voyage_words = []\n",
    "    vessel_words = []\n",
    "    flag_words = []\n",
    "    loading_words = []\n",
    "    discharge_words = []\n",
    "    first_line_below_bl_total = []\n",
    "    second_line_below_bl_total = []\n",
    "    sh_words = []\n",
    "    cn_words = []\n",
    "    no_words = []\n",
    "    regex_matched_text = []\n",
    "    #next_lines = []\n",
    "    \n",
    "    #regex_pattern = r\"(\\d{2}X\\s*\\d{2}\\'*\\s*[A-Z]+\\s*[A-Z]{3})(?:.*\\n){3}\"\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        patern=r'\\b(\\d+\\s*X\\s*\\d+\\'*[A-Za-z]+\\s*\\w+\\s*\\w+\\s*\\w+\\s*\\w+)\\b'\n",
    "        #regex_match = re.findall(r\"(\\d{2}X\\s*\\d{2}\\'*\\s*[A-Z]+\\s*[A-Z]{3})\", line)\n",
    "        regex_match = re.findall(patern, line)\n",
    "        regex_matched_text.extend(regex_match)\n",
    "        if regex_match:\n",
    "            regex_matched_text.extend(regex_match)\n",
    "            \n",
    "            # Concaténer les trois lignes suivantes dans une seule chaîne de caractères\n",
    "            next_lines = lines[i+1:i+4]\n",
    "            concatenated_lines = ' '.join(next_lines)\n",
    "            \n",
    "            regex_matched_text.append(concatenated_lines)\n",
    "        #if regex_match:\n",
    "            #regex_matched_tex = regex_match.group(1)\n",
    "            #regex_matched_text.append(\" \".join(regex_matched_tex.groups()))\n",
    "        \n",
    "        words = re.findall(r'^[A-Z]{1,4}\\d{6,}', line)\n",
    "        aev_words.extend(words)\n",
    "        \n",
    "        voyage_match = re.search(r'VOYAGE:\\s*(\\w+)', line)\n",
    "        if voyage_match:\n",
    "            voyage_word = voyage_match.group(1)\n",
    "            voyage_words.append(voyage_word)\n",
    "        \n",
    "        vessel_match = re.search(r'VESSEL:\\s*(\\w+)', line)\n",
    "        if vessel_match:\n",
    "            vessel_word = vessel_match.group(1)\n",
    "            vessel_words.append(vessel_word)\n",
    "        \n",
    "        flag_match = re.search(r'FLAG:\\s*(\\w+)', line)\n",
    "        if flag_match:\n",
    "            flag_word = flag_match.group(1)\n",
    "            flag_words.append(flag_word)\n",
    "        \n",
    "        loading_match = re.search(r'Actual Port of Loading\\s*(\\w+)', line)\n",
    "        if loading_match:\n",
    "            loading_word = loading_match.group(1)\n",
    "            loading_words.append(loading_word)\n",
    "        \n",
    "        discharge_match = re.search(r'Actual Port of Discharge\\s*(\\w+)', line)\n",
    "        if discharge_match:\n",
    "            discharge_word = discharge_match.group(1)\n",
    "            discharge_words.append(discharge_word)\n",
    "        \n",
    "        sh_match = re.search(r'SH:\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)', line)\n",
    "        if sh_match:\n",
    "            sh_words.append(\" \".join(sh_match.groups()))\n",
    "        \n",
    "        cn_match = re.search(r'CN:\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)', line)\n",
    "        if cn_match:\n",
    "            cn_words.append(\" \".join(cn_match.groups()))\n",
    "        \n",
    "        no_match = re.search(r'^N:\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)', line)\n",
    "        if no_match:\n",
    "            no_words.append(\" \".join(no_match.groups()))\n",
    "        \n",
    "        if \"ITEMS:\" in line:\n",
    "            first_line_below_bl_total.append(line)\n",
    "        \n",
    "        if \"SPLITS:\" in line:\n",
    "            second_line_below_bl_total.append(line)\n",
    "        \n",
    "        ## Récupérer le format \"12X40 ST STC\" et les trois lignes suivantes\n",
    "       # regex_match = re.search(r\"(\\d{2}X\\s*\\d{2}\\'*\\s*[A-Z]+\\s*[A-Z]{3})(?:.*\\n){3}\", line)\n",
    "        #if regex_match:\n",
    "            #regex_matched_tex = regex_match.group(1)\n",
    "            #regex_matched_text.append(\" \".join(regex_matched_tex.groups()))\n",
    "            # Récupérer les trois lignes suivantes\n",
    "            #next_lines = lines[i+1:i+4]\n",
    "    \n",
    "    if not aev_words:\n",
    "        aev_words.append(\"N/A\")\n",
    "    if not voyage_words:\n",
    "        voyage_words.append(\"N/A\")\n",
    "    if not vessel_words:\n",
    "        vessel_words.append(\"N/A\")\n",
    "    if not flag_words:\n",
    "        flag_words.append(\"N/A\")\n",
    "    if not loading_words:\n",
    "        loading_words.append(\"N/A\")\n",
    "    if not discharge_words:\n",
    "        discharge_words.append(\"N/A\")\n",
    "    if not first_line_below_bl_total:\n",
    "        first_line_below_bl_total.append(\"N/A\")\n",
    "    if not second_line_below_bl_total:\n",
    "        second_line_below_bl_total.append(\"N/A\")\n",
    "    if not sh_words:\n",
    "        sh_words.append(\"N/A\")\n",
    "    if not cn_words:\n",
    "        cn_words.append(\"N/A\")\n",
    "    if not no_words:\n",
    "        no_words.append(\"N/A\")\n",
    "    if not regex_matched_text:\n",
    "        regex_matched_text.append(\"N/A\")\n",
    "    #if not next_lines:\n",
    "        # next_lines.append(\"N/A\")\n",
    "    \n",
    "    \n",
    "    return (\n",
    "        aev_words, voyage_words, vessel_words, flag_words, loading_words,\n",
    "        discharge_words, first_line_below_bl_total, second_line_below_bl_total,\n",
    "        sh_words, cn_words, no_words, regex_matched_text\n",
    "    )\n",
    "\n",
    "\n",
    "# Chemin vers le dossier contenant les fichiers PDF\n",
    "dossier_pdf = 'C:\\\\Users\\\\ceiba\\\\Desktop\\\\PDF_EXTRACT\\\\PDF'\n",
    "# Chemin vers le dossier contenant les fichiers XML\n",
    "dossier_xml ='C:\\\\Users\\\\ceiba\\\\Desktop\\\\PDF_EXTRACT\\\\PDF'\n",
    "# Chemin vers le dossier contenant les fichiers CSV\n",
    "dossier_csv = 'C:\\\\Users\\\\ceiba\\\\Desktop\\\\PDF_EXTRACT\\\\PDF'\n",
    "\n",
    "# Rechercher tous les fichiers PDF dans le dossier\n",
    "chemins_pdf = glob.glob(os.path.join(dossier_pdf, '*.pdf'))\n",
    "\n",
    "# Parcourir chaque fichier PDF trouvé\n",
    "for chemin_pdf in chemins_pdf:\n",
    "    # Convertir les pages PDF en images en utilisant pdf2image\n",
    "    images = convert_from_path(chemin_pdf, dpi=600)\n",
    "    # Ouvrir le fichier PDF avec PDFplumber\n",
    "    with pdfplumber.open(chemin_pdf) as pdf:\n",
    "        aev_words_list = []\n",
    "        voyage_words_list = []\n",
    "        vessel_words_list = []\n",
    "        flag_words_list = []\n",
    "        loading_words_list = []\n",
    "        discharge_words_list = []\n",
    "        first_line_below_bl_total_list = []\n",
    "        second_line_below_bl_total_list = []\n",
    "        sh_words_list = []\n",
    "        cn_words_list = []\n",
    "        no_words_list = []\n",
    "        regex_matched_text_list = []\n",
    "        #next_lines_list = []\n",
    "        \n",
    "        \n",
    "        for page_number, page in enumerate(pdf.pages):\n",
    "            page_aev_words, page_voyage_words, page_vessel_words, page_flag_words, page_loading_words, page_discharge_words, page_first_line_below_bl_total, page_second_line_below_bl_total, page_sh_words, page_cn_words, page_no_words, page_regex_matched_text = extract_from_page(page_number, page)\n",
    "            aev_words_list.extend(page_aev_words)\n",
    "            voyage_words_list.extend(page_voyage_words)\n",
    "            vessel_words_list.extend(page_vessel_words)\n",
    "            flag_words_list.extend(page_flag_words)\n",
    "            loading_words_list.extend(page_loading_words)\n",
    "            discharge_words_list.extend(page_discharge_words)\n",
    "            first_line_below_bl_total_list.extend(page_first_line_below_bl_total)\n",
    "            second_line_below_bl_total_list.extend(page_second_line_below_bl_total)\n",
    "            sh_words_list.extend(page_sh_words)\n",
    "            cn_words_list.extend(page_cn_words)\n",
    "            no_words_list.extend(page_no_words)\n",
    "            regex_matched_text_list.extend(page_regex_matched_text)\n",
    "            #next_lines_list.extend(page_next_lines)\n",
    "            \n",
    "            # Vérifier la longueur des listes\n",
    "            list_lengths = [len(aev_words_list), len(voyage_words_list), len(vessel_words_list), len(flag_words_list), len(loading_words_list), len(discharge_words_list), len(first_line_below_bl_total_list), len(second_line_below_bl_total_list), len(sh_words_list), len(cn_words_list), len(no_words_list), len(regex_matched_text_list)]\n",
    "            max_length = max(list_lengths)\n",
    "            \n",
    "            # Remplir les listes manquantes avec \"N/A\" pour atteindre la même longueur\n",
    "            aev_words_list += [\"N/A\"] * (max_length - len(aev_words_list))\n",
    "            voyage_words_list += [\"N/A\"] * (max_length - len(voyage_words_list))\n",
    "            vessel_words_list += [\"N/A\"] * (max_length - len(vessel_words_list))\n",
    "            flag_words_list += [\"N/A\"] * (max_length - len(flag_words_list))\n",
    "            loading_words_list += [\"N/A\"] * (max_length - len(loading_words_list))\n",
    "            discharge_words_list += [\"N/A\"] * (max_length - len(discharge_words_list))\n",
    "            first_line_below_bl_total_list += [\"N/A\"] * (max_length - len(first_line_below_bl_total_list))\n",
    "            second_line_below_bl_total_list += [\"N/A\"] * (max_length - len(second_line_below_bl_total_list))\n",
    "            sh_words_list += [\"N/A\"] * (max_length - len(sh_words_list))\n",
    "            cn_words_list += [\"N/A\"] * (max_length - len(cn_words_list))\n",
    "            no_words_list += [\"N/A\"] * (max_length - len(no_words_list))\n",
    "            regex_matched_text_list += [\"N/A\"] * (max_length - len(regex_matched_text_list))\n",
    "            #next_lines_list += [\"N/A\"] * (max_length - len(next_lines_list))\n",
    "            \n",
    "        # Créer un DataFrame à partir des listes de mots AEV0188950, Voyage, Vessel, Flag et Actual Port of Loading\n",
    "    df = pd.DataFrame({\"Voyage\": voyage_words_list, \"BL\": aev_words_list, \"Vessel\": vessel_words_list, \"Flag\": flag_words_list, \"Actual_Port_of_Loading\": loading_words_list, \"Actual_Port_of_discharge\": discharge_words_list, \"Number_line1\": first_line_below_bl_total_list, \"Number_line2\": second_line_below_bl_total_list, \"Shipper\": sh_words_list, \"Consigner\": cn_words_list, \"Notifie\": no_words_list, \"Description\" : regex_matched_text_list})\n",
    "    # Diviser le champ \"Nom Complet\" en prénom, deuxième prénom, nom de famille et suffixe\n",
    "    df[['ITEMS', 'Type_contener', 'PACKS', 'Nbre_colis', 'WGT', 'Poids_march','Unite_poids']] = df['Number_line1'].str.split(\" \", 6, expand=True)\n",
    "    df[['SPLITS', 'Nbre_contener', 'IN_PACKS', 'Nbre_INPACKS', 'VOL', 'Volum_march','Unite_volum']] = df['Number_line2'].str.split(\" \", 6, expand=True)\n",
    "    # Supprimer les champs \"Prénom\" et \"Nom de Famille\"\n",
    "    champs_a_supprimer = [\"ITEMS\", \"PACKS\", \"WGT\", \"Number_line1\", \"SPLITS\", \"IN_PACKS\", \"Nbre_INPACKS\", \"VOL\", \"Number_line2\"]\n",
    "    df = df.drop(columns=champs_a_supprimer)\n",
    "    # Extraire le nom du fichier\n",
    "    Id_fichier = os.path.basename(chemin_pdf)\n",
    "    # Ajouter la colonne \"Nom du Fichier\" avec la valeur du nom du fichier\n",
    "    df['Nom_du_Fichier'] = Id_fichier\n",
    "############################################################################################\n",
    "    last_bl_index = df['BL'].last_valid_index()\n",
    "# Parcourir la colonne \"BL\" et remplacer chaque \"N/A\" par la dernière valeur non nulle\n",
    "    for i, value in enumerate(df['BL']):\n",
    "        if value == \"N/A\":\n",
    "            df.at[i, 'BL'] = df.at[last_bl_index, 'BL']\n",
    "        else:\n",
    "            last_bl_index = i  # Mettre à jour l'index de la dernière valeur non nulle\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "   \n",
    "    # chaque occurrence de \"N/A\" sera remplacée par la dernière valeur non nulle trouvée avant celle-ci.\n",
    "    df = df.groupby(\"BL\").agg({\n",
    "        \"Voyage\": \"first\", \n",
    "        \"Vessel\": \"first\", \n",
    "        \"Flag\": \"first\",  \n",
    "        \"Actual_Port_of_Loading\": \"first\", \n",
    "        \"Actual_Port_of_discharge\": \"first\", \n",
    "        \"Shipper\": \"first\",  \n",
    "        \"Consigner\": \"first\", \n",
    "        \"Notifie\": \"first\", \n",
    "        \"Description\":lambda x: list(set(x)),  # Prendre les valeurs uniques de \"Description\" pour chaque groupe \"BL\"  \n",
    "        \"Type_contener\": \"last\", \n",
    "        \"Nbre_colis\": \"last\", \n",
    "        \"Poids_march\": \"last\",  \n",
    "        \"Unite_poids\": \"last\", \n",
    "        \"Nbre_contener\": \"last\",   \n",
    "        \"Volum_march\": \"last\",\n",
    "        \"Unite_volum\": \"last\",\n",
    "        \"Nom_du_Fichier\": \"last\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Convertir les listes en chaînes de caractères dans le DataFrame\n",
    "    #df['Description'] = ' '.join(map(str, df['Description']))\n",
    "    df['Description'] = df['Description'].apply(lambda liste: ' '.join(map(str, liste)))\n",
    "    \n",
    "    ######################################################################################################\n",
    "    root = ET.Element('manifest')\n",
    "    \n",
    "    # Conversion du dataframe en XML\n",
    "    for _, row in df.iterrows():\n",
    "        result = ET.SubElement(root, 'Result')\n",
    "        \n",
    "        ET.SubElement(result, 'Voyage').text = row['Voyage']\n",
    "        ET.SubElement(result, 'BL').text = row['BL']\n",
    "        ET.SubElement(result, 'Vessel').text = row['Vessel']\n",
    "        ET.SubElement(result, 'Flag').text = row['Flag']\n",
    "        ET.SubElement(result, 'Actual_Port_of_Loading').text = row['Actual_Port_of_Loading']\n",
    "        ET.SubElement(result, 'Actual_Port_of_discharge').text = row['Actual_Port_of_discharge']\n",
    "        ET.SubElement(result, 'Nbre_colis').text = row['Nbre_colis']\n",
    "        ET.SubElement(result, 'Poids_march').text = row['Poids_march']\n",
    "        ET.SubElement(result, 'Unite_poids').text = row['Unite_poids']\n",
    "        ET.SubElement(result, 'Nbre_contener').text = row['Nbre_contener']\n",
    "        ET.SubElement(result, 'Type_contener').text = row['Type_contener']\n",
    "        ET.SubElement(result, 'Volum_march').text = row['Volum_march']\n",
    "        ET.SubElement(result, 'Unite_volum').text = row['Unite_volum']\n",
    "        ET.SubElement(result, 'Shipper').text = row['Shipper']\n",
    "        ET.SubElement(result, 'Consigner').text = row['Consigner']\n",
    "        ET.SubElement(result, 'Notifie').text = row['Notifie']\n",
    "        ET.SubElement(result, 'Description').text = row['Description']\n",
    "        \n",
    "    # On peut ajouter d'autres éléments en fonction de la structure de votre dataframe\n",
    "    \n",
    "    # Enregistrer le XML dans un fichier avec un nom basé sur le fichier PDF\n",
    "    nom_fichier_xml = os.path.splitext(os.path.basename(chemin_pdf))[0] + '.xml'\n",
    "    chemin_fichier_xml = os.path.join(dossier_xml, nom_fichier_xml)\n",
    "    # Création de l'arbre XML\n",
    "    xml_tree = ET.ElementTree(root)\n",
    "    # Enregistrement du XML dans un fichier\n",
    "    xml_tree.write(chemin_fichier_xml, encoding='utf-8', xml_declaration=True)\n",
    "    \n",
    "    \n",
    "    # Enregistrer le DataFrame en tant que fichier CSV avec un nom basé sur le fichier PDF\n",
    "    nom_fichier_csv = os.path.splitext(os.path.basename(chemin_pdf))[0] + '.csv'\n",
    "    chemin_fichier_csv = os.path.join(dossier_csv, nom_fichier_csv)\n",
    "    df.to_csv(chemin_fichier_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6a5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4af0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Exemple de DataFrame avec des listes dans les cellules\n",
    "donnees = {\n",
    "    'colonne1': [1, 2, 3],\n",
    "    'colonne2': ['a', 'b', 'c'],\n",
    "    'colonne_liste': [[10, 20, 30], [40, 50], [60, 70, 80]]\n",
    "}\n",
    "\n",
    "mon_dataframe = pd.DataFrame(donnees)\n",
    "\n",
    "# Convertir les listes en chaînes de caractères dans le DataFrame\n",
    "mon_dataframe['colonne_liste'] = mon_dataframe['colonne_liste'].apply(lambda liste: ' '.join(map(str, liste)))\n",
    "\n",
    "# Écrire le DataFrame dans un fichier CSV\n",
    "mon_dataframe.to_csv('mon_fichier.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
